{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.utils.export import generate_multimodal_pages\n",
    "from docling.utils.utils import create_hash\n",
    "import logging\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 3.0\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.document_converter import (\n",
    "    DocumentConverter,\n",
    "    PdfFormatOption,\n",
    "    WordFormatOption,\n",
    ")\n",
    "from docling.pipeline.simple_pipeline import SimplePipeline\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "\n",
    "doc_converter = (\n",
    "    DocumentConverter(  # all of the below is optional, has internal defaults.\n",
    "        allowed_formats=[\n",
    "            InputFormat.PDF,\n",
    "            InputFormat.IMAGE,\n",
    "            InputFormat.DOCX,\n",
    "            InputFormat.HTML,\n",
    "            InputFormat.PPTX,\n",
    "            InputFormat.ASCIIDOC,\n",
    "            InputFormat.MD,\n",
    "        ],  # whitelist formats, non-matching files are ignored.\n",
    "        format_options={\n",
    "        #     InputFormat.PDF: PdfFormatOption(\n",
    "        #         pipeline_cls=StandardPdfPipeline, backend=PyPdfiumDocumentBackend\n",
    "        #     )\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        ,\n",
    "            InputFormat.DOCX: WordFormatOption(\n",
    "                pipeline_cls=SimplePipeline  # , backend=MsWordDocumentBackend\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm_llama3 = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "llm_llava = OllamaLLM(model=\"llava\", \n",
    "    temperature=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import uuid\n",
    "import os\n",
    "from io import BytesIO\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(r\"D:\\Ed\\RAGs\\PDF_breaking\\Manual fr RAG.pdf\")\n",
    "fldr_nm = \"_\".join(file_path.name.split('.')[:-1])\n",
    "output_dir = Path(f\"./docling/{fldr_nm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_input):\n",
    "    \"\"\"\n",
    "    Convert image file path or PIL image to Base64 encoded string.\n",
    "\n",
    "    :param image_input: Path to the image file or a PIL image object\n",
    "    :return: Base64 encoded string\n",
    "    \"\"\"\n",
    "    if isinstance(image_input, str):\n",
    "        # Assume the input is a file path\n",
    "        with open(image_input, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        # Assume the input is a PIL image\n",
    "        buffered = BytesIO()\n",
    "        image_input.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "        return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported input type. Please provide a file path or a PIL image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_results = doc_converter.convert_all([file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{output_dir}/images/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama_utils import ollama_model_list, ollama_active_model_list, ollama_unload_models, stop_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Set the timeout to 60 seconds\n",
    "timeout_duration = 60\n",
    "\n",
    "# Function to execute your code with a timeout and retry mechanism\n",
    "def execute_with_timeout_and_retry():\n",
    "    for image_b64 in img_base64_list:\n",
    "        retries = 3\n",
    "        success = False\n",
    "        while retries > 0 and not success:\n",
    "            def target():\n",
    "                try:\n",
    "                    llm_with_image_context = llm_llava.bind(images=[image_b64])\n",
    "                    mdl_rstl = llm_with_image_context.invoke(img_smry_prompt)\n",
    "                    image_summaries.append(mdl_rstl)\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "            thread = threading.Thread(target=target)\n",
    "            thread.start()\n",
    "            thread.join(timeout_duration)\n",
    "            \n",
    "            if thread.is_alive():\n",
    "                print(f\"Execution timed out! Retries left: {retries - 1}\")\n",
    "                thread.join(0)  # Kill the thread\n",
    "                retries -= 1\n",
    "            else:\n",
    "                success = True\n",
    "\n",
    "# Call the function\n",
    "execute_with_timeout_and_retry()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@eric_vaillancourt/mastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# sntnc_trnsfrmr_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=sntnc_trnsfrmr_embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000247C8D9FE20>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualize_q_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm_llama3, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt_text = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chain = rag_prompt_text | llm_llama3 | StrOutputParser()\n",
    "retrieval_chain = {\"context\": history_aware_retriever, \"question\": RunnablePassthrough()\n",
    "                    } | RunnablePassthrough.assign(output=generation_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mretrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Task Decomposition?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3725\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3720\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3721\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3722\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3723\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3724\u001b[0m         ]\n\u001b[1;32m-> 3725\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3726\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3709\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3707\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   3708\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m-> 3709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3713\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\branch.py:223\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, branch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranches):\n\u001b[0;32m    221\u001b[0m     condition, runnable \u001b[38;5;241m=\u001b[39m branch\n\u001b[1;32m--> 223\u001b[0m     expression_value \u001b[38;5;241m=\u001b[39m \u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcondition:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expression_value:\n\u001b[0;32m    232\u001b[0m         output \u001b[38;5;241m=\u001b[39m runnable\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    234\u001b[0m             config\u001b[38;5;241m=\u001b[39mpatch_config(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    239\u001b[0m         )\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4711\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4697\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4698\u001b[0m \n\u001b[0;32m   4699\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4708\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4717\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4718\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4719\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4721\u001b[0m     )\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1925\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1922\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1923\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1924\u001b[0m         Output,\n\u001b[1;32m-> 1925\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1933\u001b[0m     )\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1935\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4565\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4563\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4565\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4566\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   4567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4568\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\langchain\\chains\\history_aware_retriever.py:60\u001b[0m, in \u001b[0;36mcreate_history_aware_retriever.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `input` to be a prompt variable, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m retrieve_documents: RetrieverOutputLike \u001b[38;5;241m=\u001b[39m RunnableBranch(\n\u001b[0;32m     58\u001b[0m     (\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;66;03m# Both empty string and empty list evaluate to False\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# If no chat history, then we just pass input to retriever\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m|\u001b[39m retriever,\n\u001b[0;32m     63\u001b[0m     ),\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# If chat history, then we pass inputs to LLM chain, then to retriever\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser() \u001b[38;5;241m|\u001b[39m retriever,\n\u001b[0;32m     66\u001b[0m )\u001b[38;5;241m.\u001b[39mwith_config(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_retriever_chain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_documents\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your chain to store the documents and extract the source\n",
    "context = (\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"memory\": memory.load_memory_variables,\n",
    "        }\n",
    "    )\n",
    "    | RunnableMap(\n",
    "    {\n",
    "        \"context\": (\n",
    "            retriever_chain\n",
    "            | _get_k_or_less_documents\n",
    "            | (lambda docs: [documents.append(doc.metadata['source']) for doc in docs] or docs)  # Store the source of the documents\n",
    "            | reorder_documents\n",
    "            | format_docs\n",
    "        ),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"memory\"][\"chat_history\"],\n",
    "    }\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# chat_history = []\n",
    "\n",
    "# question = \"What is Task Decomposition?\"\n",
    "# chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "# second_question = \"What are common ways of doing it?\"\n",
    "# ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "# print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of Large Language Models (LLMs), \"large\" refers to the size or scope of the model, specifically in terms of its computational resources and data usage. A Large Language Model typically involves:\\n\\n1. **Training data**: The model is trained on a massive dataset of text, often containing billions of words, phrases, or even entire books.\\n2. **Computational power**: The model requires significant processing power to analyze and process the vast amount of data it\\'s trained on.\\n3. **Model complexity**: LLMs are typically complex models with many layers, which enables them to learn intricate patterns in language.\\n\\nIn essence, a Large Language Model is an extremely powerful tool that can generate human-like text, answer complex questions, or even engage in conversations with humans.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "contextualize_q_system_prompt = \"\"\"You are a question formulation engine. Formulate a standalone question combining the chat_history and the latest user question.\n",
    "Strictly Do not answer the question. Just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm_llama3 | StrOutputParser()\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_llm_cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We can do the same thing with a SQLite cache\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLiteCache\n\u001b[1;32m----> 4\u001b[0m \u001b[43mset_llm_cache\u001b[49m(SQLiteCache(database_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.langchain.db\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'set_llm_cache' is not defined"
     ]
    }
   ],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cache_vector_retriever(\n",
    "    vectorstore, question, qas\n",
    "):\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"qa_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "        similarity_score_threshold = 0.9,\n",
    "        # search_type=\"mmr\", \n",
    "        # search_kwargs={\"k\": 3}\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add the question and answers to the retriever\n",
    "    add_documents(retriever, question, qas)\n",
    "        \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "sntnc_trnsfrmr_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_cache_vector = Chroma(persist_directory=\"./src/vector_dbs/chroma_cache_vector\", embedding_function=sntnc_trnsfrmr_embeddings)\n",
    "cache_path = r\"D:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\src\\vector_dbs\\src\\vector_dbs\\chroma_cache_vector\"\n",
    "chroma_cache_vector = Chroma(persist_directory=cache_path, embedding_function=sntnc_trnsfrmr_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dict = {\n",
    "    \"What is the capital of France?\": \"Paris\",\n",
    "    \"What is 2 + 2?\": \"4\",\n",
    "    \"Who wrote 'Harry Potter'?\": \"J.K. Rowling\",\n",
    "    \"What is the largest planet in our solar system?\": \"Jupiter\",\n",
    "    \"How many continents are there on Earth?\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = list(qa_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the capital of France?',\n",
       " 'What is 2 + 2?',\n",
       " \"Who wrote 'Harry Potter'?\",\n",
       " 'What is the largest planet in our solar system?',\n",
       " 'How many continents are there on Earth?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"Who wrote 'Harry Potter'?\", \"answer\": \"J.K. Rowling\"},\n",
    "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
    "    {\"question\": \"How many continents are there on Earth?\", \"answer\": \"7\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = [ {ky:vl} for ky, vl in qa_dict.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'What is the capital of France?': 'Paris'},\n",
       " {'What is 2 + 2?': '4'},\n",
       " {\"Who wrote 'Harry Potter'?\": 'J.K. Rowling'},\n",
       " {'What is the largest planet in our solar system?': 'Jupiter'},\n",
       " {'How many continents are there on Earth?': '7'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetriver_cache = create_cache_vector_retriever(\n",
    "    chroma_cache_vector, question, qas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'What is the capital of France?': 'Paris'},\n",
       " {'How many continents are there on Earth?': '7'},\n",
       " {'What is the largest planet in our solar system?': 'Jupiter'},\n",
       " {\"Who wrote 'Harry Potter'?\": 'J.K. Rowling'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetriver_cache.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'What is the capital of France?': 'Paris'},\n",
       " {'How many continents are there on Earth?': '7'},\n",
       " {'What is the largest planet in our solar system?': 'Jupiter'},\n",
       " {\"Who wrote 'Harry Potter'?\": 'J.K. Rowling'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetriver_cache.invoke(\"What is the france capital?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "docstore = InMemoryStore()\n",
    "fake_whole_documents = [\n",
    "    (\"fake_id_1\", Document(page_content=\"fake whole document 1\")),\n",
    "    (\"fake_id_2\", Document(page_content=\"fake whole document 2\")),\n",
    "]\n",
    "docstore.mset(fake_whole_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiVectorRetriever\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackManagerForRetrieverRun\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mCustomMultiVectorRetriever\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mMultiVectorRetriever\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m_get_relevant_documents\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallbackManagerForRetrieverRun\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Get documents relevant to a query.\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;43;03m        Args:\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;43;03m            query: String to find relevant documents for\u001b[39;49;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;43;03m            List of relevant documents\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;43;03m        \"\"\"\u001b[39;49;00m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mCustomMultiVectorRetriever\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCustomMultiVectorRetriever\u001b[39;00m(MultiVectorRetriever):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m---> 10\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m[Document]:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Get documents relevant to a query.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m            query: String to find relevant documents for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m            List of relevant documents\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m     19\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m     20\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "\n",
    "class CustomMultiVectorRetriever(MultiVectorRetriever):\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(\n",
    "            query, **self.search_kwargs\n",
    "        )\n",
    "\n",
    "        # Map doc_ids to list of sub-documents, adding scores to metadata\n",
    "        id_to_doc = defaultdict(list)\n",
    "        for doc, score in results:\n",
    "            doc_id = doc.metadata.get(\"doc_id\")\n",
    "            if doc_id:\n",
    "                doc.metadata[\"score\"] = score\n",
    "                id_to_doc[doc_id].append(doc)\n",
    "\n",
    "        # Fetch documents corresponding to doc_ids, retaining sub_docs in metadata\n",
    "        docs = []\n",
    "        for _id, sub_docs in id_to_doc.items():\n",
    "            docstore_docs = self.docstore.mget([_id])\n",
    "            if docstore_docs:\n",
    "                if doc := docstore_docs[0]:\n",
    "                    doc.metadata[\"sub_docs\"] = sub_docs\n",
    "                    docs.append(doc)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiVectorRetriever' object has no attribute 'similarity_search_with_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvetriver_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_scores\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiVectorRetriever' object has no attribute 'similarity_search_with_scores'"
     ]
    }
   ],
   "source": [
    "vetriver_cache.similarity_search_with_scores(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The storage layer for the parent documents\n",
    "docstore = InMemoryStore()\n",
    "fake_whole_documents = [\n",
    "    (\"fake_id_1\", Document(page_content=\"fake whole document 1\")),\n",
    "    (\"fake_id_2\", Document(page_content=\"fake whole document 2\")),\n",
    "]\n",
    "docstore.mset(fake_whole_documents)\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing cats.\",\n",
    "        metadata={\"doc_id\": \"fake_id_1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing discourse.\",\n",
    "        metadata={\"doc_id\": \"fake_id_1\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A snippet from a larger document discussing chocolate.\",\n",
    "        metadata={\"doc_id\": \"fake_id_2\"},\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_retriver.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomMultiVectorRetriever(MultiVectorRetriever):\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(\n",
    "            query, **self.search_kwargs\n",
    "        )\n",
    "\n",
    "        # Map doc_ids to list of sub-documents, adding scores to metadata\n",
    "        id_to_doc = defaultdict(list)\n",
    "        for doc, score in results:\n",
    "            doc_id = doc.metadata.get(\"doc_id\")\n",
    "            if doc_id:\n",
    "                doc.metadata[\"score\"] = score\n",
    "                id_to_doc[doc_id].append(doc)\n",
    "\n",
    "        # Fetch documents corresponding to doc_ids, retaining sub_docs in metadata\n",
    "        docs = []\n",
    "        for _id, sub_docs in id_to_doc.items():\n",
    "            docstore_docs = self.docstore.mget([_id])\n",
    "            if docstore_docs:\n",
    "                if doc := docstore_docs[0]:\n",
    "                    doc.metadata[\"sub_docs\"] = sub_docs\n",
    "                    docs.append(doc)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'sub_docs': [Document(id='5bb86d23-c07d-4142-917b-8c3cedea51a6', metadata={'doc_id': 'fake_id_1', 'score': 0.6802923421811876}, page_content='A snippet from a larger document discussing cats.'), Document(id='c32eaea2-f7ff-4014-b193-6c78d8845768', metadata={'doc_id': 'fake_id_1', 'score': 1.603668937332378}, page_content='A snippet from a larger document discussing discourse.')]}, page_content='fake whole document 1'),\n",
       " Document(metadata={'sub_docs': [Document(id='4abe6165-1049-41a3-8ddf-b8b4de85eead', metadata={'doc_id': 'fake_id_2', 'score': 1.5159255288970122}, page_content='A snippet from a larger document discussing chocolate.')]}, page_content='fake whole document 2')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = CustomMultiVectorRetriever(vectorstore=baseline_retriver, docstore=docstore)\n",
    "retriever.invoke(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom multi retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class sentence_transform_HuggingFaceEmbeddings(HuggingFaceEmbeddings):\n",
    "    # ... (rest of your class definition)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a HuggingFace transformer model.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of normalized embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        import sentence_transformers  # type: ignore[import]\n",
    "\n",
    "        texts = list(map(lambda x: x.replace(\"\\n\", \" \"), texts))\n",
    "        if self.multi_process:\n",
    "            pool = self._client.start_multi_process_pool()\n",
    "            embeddings = self._client.encode_multi_process(texts, pool)\n",
    "            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n",
    "        else:\n",
    "            embeddings = self._client.encode(\n",
    "                texts,\n",
    "                show_progress_bar=self.show_progress,\n",
    "                **self.encode_kwargs,  # type: ignore\n",
    "            )\n",
    "\n",
    "        if isinstance(embeddings, list):\n",
    "            raise TypeError(\n",
    "                \"Expected embeddings to be a Tensor or a numpy array, \"\n",
    "                \"got a list instead.\"\n",
    "            )\n",
    "\n",
    "        # Convert embeddings to a tensor\n",
    "        embeddings_tensor = torch.tensor(embeddings)\n",
    "\n",
    "        # Normalize the embeddings using torch.nn.functional\n",
    "        normalized_embeddings = F.normalize(embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        return normalized_embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Compute query embeddings using a HuggingFace transformer model.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Normalized embeddings for the text.\n",
    "        \"\"\"\n",
    "        return self.embed_documents([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016221188008785248, -0.09838076680898666, -0.011730898171663284, 0.013730788603425026, -0.06682492047548294, 0.017713487148284912, 0.011615675874054432, 0.03769439458847046, -0.0032387785613536835, -0.04462600499391556, 0.0933019295334816, -0.01635119505226612, 0.01875177025794983, -0.018939128145575523, 0.0068094427697360516, -0.07247532904148102, 0.03482732176780701, 0.0033719358034431934, -0.002407501684501767, 0.015350432135164738, 0.0030232081189751625, 0.045000627636909485, -0.005792027339339256, 0.01522165909409523, 0.04382133483886719, -0.01855294406414032, -0.0004711848741862923, -0.02047121152281761, 0.011981245130300522, -0.04444706439971924, 0.007604053709656, 0.004922519903630018, -0.02299950271844864, -0.0833158865571022, 1.5827598645046237e-06, -0.012971579097211361, -0.032535530626773834, -0.0022125772666186094, -0.013456594198942184, 0.01828291453421116, 0.06450267881155014, 0.07536079734563828, -0.02602248452603817, 0.0492517463862896, -0.009309137240052223, 0.007645824924111366, 0.059947699308395386, 0.03187352418899536, -0.02222226746380329, 0.05206512659788132, 0.00030969351064413786, -0.04498927667737007, -0.01852363534271717, -0.026168765500187874, 0.04462658241391182, 0.02431202307343483, 0.016048511490225792, -0.014855926856398582, 0.031910721212625504, 0.04756851866841316, 0.030355006456375122, -0.0036157190334051847, -0.03406713157892227, -0.004686055239289999, -0.020246827974915504, -0.012836071662604809, -0.03959242254495621, -0.007909677922725677, 0.01558130793273449, 0.03072539158165455, 0.10022758692502975, -0.026319507509469986, 0.02184714563190937, 0.02511964924633503, -0.010429254733026028, 0.023420080542564392, -0.029236141592264175, 0.0041312845423817635, -0.03797660395503044, 0.016175197437405586, 0.036634232848882675, 0.03962049260735512, -0.006704540457576513, -0.0021741848904639482, -0.06062629818916321, 0.02475239522755146, -0.020353589206933975, -0.0202731154859066, -0.023565884679555893, -0.023906514048576355, -0.015222135931253433, -0.004412567242980003, 0.007881693542003632, 0.013866476714611053, 0.09222658723592758, 0.0013131771702319384, -0.062409304082393646, -0.1299353390932083, 0.023922894150018692, -0.022440727800130844, 0.03213053569197655, -0.005928474478423595, -0.01910894736647606, 0.03019619733095169, -0.0018919752910733223, -0.04745584353804588, 0.061967648565769196, 0.010409933514893055, -0.039803195744752884, 0.04787056893110275, 0.05264972150325775, -0.02949703298509121, -0.0962061956524849, 0.0060011581517755985, -0.008578499779105186, -0.010758689604699612, 0.020128946751356125, -0.007893599569797516, 0.03929838538169861, 0.02084159292280674, -0.013102035038173199, -0.012808801606297493, -0.00038539335946552455, 0.03223637118935585, 0.0023509233724325895, -0.012976566329598427, -0.030944615602493286, 0.0016219137469306588, 0.0036385376006364822, -0.05850634351372719, 0.0017893986077979207, 0.037216562777757645, 0.049428943544626236, -0.029047111049294472, -0.040560461580753326, 0.033864326775074005, 0.01518998108804226, -0.025366051122546196, -0.02009817771613598, 0.013870223425328732, 0.011556204408407211, 0.010235981084406376, -0.01425827108323574, -0.009645967744290829, -0.026595864444971085, 0.04601817950606346, 0.021452348679304123, -0.0004135236085858196, 0.027151670306921005, 0.04989469423890114, -0.05060767009854317, 0.03642583638429642, -0.05728974938392639, -0.023887906223535538, -0.032307472079992294, -0.04310562461614609, 0.07301156967878342, -0.023132722824811935, -0.008374805562198162, -0.024512259289622307, 0.062087275087833405, 0.04428244009613991, -0.015356521122157574, 0.017634309828281403, -0.03086891397833824, -0.008794249035418034, -0.031763844192028046, -0.009598883800208569, -0.041047099977731705, 0.006094047799706459, -0.03132753446698189, 0.030391845852136612, -0.02352043241262436, 0.06993026286363602, 0.05625420808792114, 0.15752367675304413, 0.02390935830771923, 0.022227350622415543, -0.04480937123298645, 0.07037998735904694, 0.04084215685725212, -0.04416428506374359, -0.0325358510017395, 0.009417563676834106, -0.014077038504183292, 0.05355183407664299, 0.0212471354752779, -0.004459195304661989, 0.0030180078465491533, -0.02677767723798752, -0.04512893408536911, 0.0379813127219677, -0.005653413012623787, -0.033940304070711136, -0.007219528779387474, 0.01859039068222046, -0.04220709577202797, -0.005245006177574396, -0.013301131315529346, -0.011502757668495178, 0.033700425177812576, 0.020132537931203842, 0.05259643867611885, 0.0951152890920639, 0.013455691747367382, -0.03151075169444084, 0.0855073407292366, -0.05789710208773613, 0.010535098612308502, 0.0417637974023819, 0.05069589987397194, 0.029839880764484406, -0.01645680144429207, -0.028655393049120903, 0.00421532616019249, -0.00728348083794117, 0.022593360394239426, -0.01794409565627575, -0.05434665456414223, 0.02873491495847702, -0.018282637000083923, -0.02364051155745983, -0.058091167360544205, 0.0331718847155571, 0.04251648113131523, -0.06495413184165955, -0.030402155593037605, -0.01136058010160923, -0.018785538151860237, -0.0011963400756940246, -0.06532204151153564, 0.031217500567436218, -0.0015583590138703585, 0.03543280065059662, -0.026582054793834686, 0.04181826487183571, 0.008600348606705666, -0.024333452805876732, -0.015064206905663013, -0.0996849313378334, 0.019512493163347244, 0.01500812079757452, 0.03616536781191826, 0.03411328047513962, -0.024569829925894737, -0.019860001280903816, 0.05250013992190361, -0.035017188638448715, 0.024273212999105453, -0.016722755506634712, -0.00020303168275859207, -0.03529893606901169, -0.004330614116042852, 0.016896409913897514, 0.02981860563158989, 0.002050749259069562, -0.004122503101825714, 0.013631169684231281, 0.009053695946931839, -0.020931122824549675, 0.02694902941584587, -0.041058991104364395, -0.0018015344394370914, 0.027570340782403946, -0.019502993673086166, 0.013432315550744534, -0.023029666393995285, -0.04383654519915581, -0.04251653701066971, -0.029466433450579643, -0.011856780387461185, 0.03591030463576317, 0.008507771417498589, 0.006878992542624474, 0.04432761296629906, 0.002978874836117029, -0.034178148955106735, -0.019688986241817474, -0.009078560397028923, 0.042852502316236496, -0.011957363225519657, 0.017665207386016846, -0.03554831072688103, 0.007126040291041136, -0.0033726864494383335, 0.03506387025117874, 0.032454170286655426, -0.047085534781217575, -0.047234971076250076, 0.045293208211660385, 0.047706108540296555, 0.017411846667528152, 0.03816977143287659, -0.03818770870566368, 0.02414204739034176, -0.03212927281856537, 0.034477271139621735, 0.05082712695002556, 0.03708408772945404, 0.027673689648509026, -0.008452552370727062, -0.021713862195611, 0.024047214537858963, -0.02825697511434555, -0.015373626723885536, 0.08174384385347366, 0.011684811674058437, -0.09051978588104248, 0.005231106653809547, 0.03480492904782295, 0.058498576283454895, 0.011363567784428596, -0.00252513331361115, 0.015713753178715706, -0.026681184768676758, -0.0020249877125024796, -0.0047432007268071175, -0.01228252798318863, -0.08111941814422607, 0.013625407591462135, -0.010100583545863628, 0.019022291526198387, -0.013572468422353268, -0.005458375439047813, 0.02852156199514866, -0.040389783680438995, -0.015705682337284088, 0.010905411094427109, -0.016581157222390175, -0.027041183784604073, 0.0013207209995016456, 0.03159862384200096, -0.08098846673965454, 0.06647216528654099, 0.03680445998907089, 0.05555863305926323, -0.023867405951023102, 0.056502074003219604, -0.012721328996121883, -0.01098713744431734, 0.0071612875908613205, 2.8115848181187175e-05, 0.025243252515792847, -0.04728052020072937, 0.012388641014695168, -0.0033452510833740234, 0.02059406228363514, -0.0006868198979645967, -0.07721094787120819, -0.00026623590383678675, 0.022064583376049995, 0.06602242588996887, -0.017735546454787254, -0.0007228836766444147, -0.061566293239593506, 0.018226739019155502, 0.010417954064905643, -0.03143753111362457, -0.005560536868870258, -0.06365243345499039, -0.008249900303781033, 0.015390326268970966, -0.026446351781487465, 0.033829059451818466, 0.006266668438911438, -0.028740162029862404, 0.03181252256035805, -0.024381108582019806, -0.028544671833515167, 0.024958724156022072, -0.0033868313767015934, 0.012185386382043362, 0.007672745734453201, 0.05907897651195526, -0.011043926700949669, 0.012804563157260418, 0.005614619702100754, -0.07247773557901382, -0.00903401244431734, 0.009408165700733662, 0.011666814796626568, -0.017784008756279945, 0.04945307597517967, -0.01340060867369175, -0.019671233370900154, 0.05374741926789284, -0.022760936990380287, -0.003607151098549366, -0.007308861240744591, -0.012027406133711338, 0.021684745326638222, -0.09013208001852036, -0.04011324420571327, 0.008145040832459927, -0.006682775914669037, 0.030125409364700317, 0.004874940030276775, 0.02150251343846321, -0.024435384199023247, 0.005152867175638676, 0.028864499181509018, -0.0339256227016449, 0.025896912440657616, -0.027568072080612183, -0.040968555957078934, 0.025680461898446083, 0.013014975935220718, 0.02906699851155281, -0.0724375769495964, 0.05495842173695564, -0.0022063518408685923, -0.07891634106636047, 0.0171675905585289, 0.018384987488389015, 0.06492675840854645, -0.013399239629507065, -0.013739816844463348, 0.009074012748897076, 0.00926218181848526, -0.005843080580234528, -0.05867604911327362, -0.0006677849451079965, 0.03393552824854851, -0.013443642295897007, 0.02172144129872322, -0.004059602972120047, 0.017049264162778854, 0.022314736619591713, -0.013621315360069275, 0.012969138100743294, -0.008189864456653595, -0.03353708237409592, -0.04397175461053848, 0.017689364030957222, -0.0861968919634819, -0.066646046936512, -0.027894971892237663, -0.029324231669306755, 0.0018605811055749655, -0.06519562751054764, 0.010576639324426651, -0.011765094473958015, 0.1003275141119957, 0.04055016487836838, 0.02205924689769745, 0.043202050030231476, -0.012448466382920742, -0.01909952610731125, -0.01910368725657463, 0.02071642503142357, 0.018986115232110023, -0.0007298285490833223, -0.06513230502605438, -0.009419649839401245, 0.03709904104471207, -0.004052337259054184, -0.05800984054803848, -0.0076485928148031235, -0.03417690843343735, 0.026913056150078773, -0.014498768374323845, 0.035600271075963974, -0.07320363819599152, -0.01592649146914482, -0.03947681188583374, 0.052269887179136276, -0.011007161810994148, 0.00701950816437602, 0.013985201716423035, -0.009314977563917637, -0.027132155373692513, 0.003914965316653252, -0.01358751393854618, 0.023076634854078293, 0.0036017620004713535, -0.04474407806992531, -0.05379939451813698, -0.047865282744169235, -0.01955582946538925, 0.01874949038028717, -0.013511967845261097, -0.02110661379992962, -0.027130207046866417, -0.015362466685473919, -0.03581506386399269, -0.016133945435285568, -0.03756055235862732, 0.002290656790137291, 0.019566548988223076, 0.02045908384025097, -0.012374796904623508, -0.03415268659591675, -0.036925289779901505, 0.016095569357275963, -0.030342334881424904, 0.03900686651468277, 0.024572717025876045, 0.026608383283019066, -0.015520213171839714, -0.005910820327699184, 0.014091281220316887, 0.005985093303024769, -0.002568467054516077, -0.010581230744719505, -0.09217852354049683, 0.039634719491004944, 0.03837130218744278, 0.022368336096405983, 0.015183608047664165, 0.02236022800207138, 0.007588737178593874, -0.013947591185569763, -0.02929965779185295, 0.004453001543879509, -0.044757016003131866, -0.021663453429937363, -0.026146812364459038, -0.07562649250030518, 0.038236018270254135, -0.006623751483857632, 0.006521293427795172, -0.017862750217318535, 0.019164908677339554, -0.007541998755186796, -0.040607817471027374, -0.012225582264363766, 0.017696499824523926, 0.029912490397691727, -0.04181907698512077, 0.03128495812416077, -0.013242165558040142, -0.016948342323303223, 0.004931256640702486, 0.013348742388188839, -0.04200290888547897, -0.010495156049728394, 0.07150667905807495, 0.03892313316464424, 0.0341743528842926, -0.022924454882740974, 0.017842622473835945, -0.020344765856862068, 0.07767701148986816, 0.058331046253442764, 0.03610844910144806, -0.025911638513207436, -0.02410283498466015, 0.05475696921348572, -0.02997257374227047, -0.0257379412651062, -0.04443605989217758, 0.0705609917640686, -0.004968985915184021, 0.027090488001704216, 0.01630127988755703, -6.116469224758794e-33, -0.024562031030654907, -0.03786740079522133, -0.016574131324887276, 0.07764984667301178, -0.013178958557546139, -0.056563202291727066, -0.026145048439502716, 0.0013941549696028233, -0.009449631907045841, 0.03656900301575661, -0.016723986715078354, -0.016578400507569313, 0.03982013836503029, -0.013679420575499535, -0.0026560218539088964, 0.025961603969335556, 0.02319728396832943, -0.02293381094932556, 0.001451576012186706, -0.06838265061378479, 0.0006521325558423996, -0.04292045533657074, 0.07654869556427002, -0.028911812230944633, 0.04938185587525368, -0.050849344581365585, -0.04222819209098816, -0.05095067620277405, 0.014855601824820042, -0.012397321872413158, 0.01639505848288536, -0.0210673026740551, 0.009045085869729519, -0.0013227632734924555, -0.009892826899886131, 0.1120944544672966, -0.028438201174139977, -0.0676746740937233, 0.035521503537893295, 0.018117746338248253, -0.03440769761800766, -0.012532122433185577, -0.043080661445856094, -0.03149377927184105, -2.371022492297925e-05, -0.0054720984771847725, 0.04185611754655838, -0.043897878378629684, -0.01999548077583313, -0.06914990395307541, -0.021207135170698166, 0.02143346145749092, -0.011760870926082134, 0.08585169911384583, 0.018480334430933, 0.044780436903238297, -0.009338862262666225, -0.016338638961315155, -0.005044381134212017, 0.00780597934499383, 0.08076746761798859, 0.08426160365343094, -0.04470789059996605, -0.025271808728575706, -0.0018049615900963545, 0.0355033315718174, -0.010609070770442486, 0.01792282611131668, 0.02919737994670868, 0.025524074211716652, 0.02836127206683159, 0.03651992976665497, -0.012470985762774944, 0.1453867256641388, 0.02206547185778618, -0.025866951793432236, 0.002016206504777074, 0.021481161937117577, 0.016008930280804634, -0.009149497374892235, 0.03537026420235634, -0.03472735732793808, -0.007623008918017149, -0.04329053685069084, -0.06433414667844772, -0.05029316619038582, -0.0070573994889855385, 0.002064129803329706, -0.0009430138161405921, -0.003500564256682992, -0.012124009430408478, 0.06288918107748032, 0.0062911417335271835, -0.00801043026149273, -0.028700392693281174, 0.024949053302407265, -0.01955924928188324, 0.040991153568029404, -0.06980853527784348, -0.04186568036675453, -0.062411654740571976, -0.06223613768815994, -0.026187200099229813, -0.021181030198931694, 0.024479970335960388, -0.0071569946594536304, -0.004468018654733896, 0.0010691973147913814, -0.1309686154127121, 0.00434507243335247, 0.024741731584072113, 0.01575983315706253, 0.0518360510468483, -0.010980604216456413, -0.016091646626591682, -0.03021487221121788, 0.04418234899640083, 0.02708960697054863, -0.012345908209681511, -0.0707663744688034, 0.012303709983825684, -0.008080772124230862, -0.025350024923682213, -0.02021363563835621, 0.0008068305323831737, 0.029865577816963196, 0.019579341635107994, 0.03134140744805336, 0.05830298364162445, -0.006992881186306477, 0.005611236207187176, 0.021408434957265854, 2.159810037483112e-07, 0.04010794684290886, 0.05182873085141182, 0.05933844670653343, 0.046150751411914825, 0.024269234389066696, -0.009777936153113842, 0.03512544184923172, 0.050746481865644455, -0.014351507648825645, 0.004936683923006058, 0.05332937464118004, -0.04734932631254196, 0.007018544711172581, 0.02421398088335991, -0.08437472581863403, 0.06210489198565483, -0.04036945849657059, -0.05727770924568176, -0.00496522756293416, -0.025164833292365074, 0.04033290594816208, 0.02879379503428936, 0.025484317913651466, 0.008952906355261803, -0.007821641862392426, -0.03967610001564026, 0.0023710669483989477, -0.0474906861782074, 0.002290495205670595, 0.029763061553239822, -0.04908415302634239, -0.0685097724199295, -0.0036214718129485846, 0.03897080197930336, -0.013185771182179451, -0.01615961082279682, 0.05489097163081169, 0.06439747661352158, 0.01587197184562683, 0.07594260573387146, 0.014376228675246239, -0.059506069868803024, -0.014293674379587173, 0.007283079903572798, 0.04089512303471565, 0.046086519956588745, 0.041769206523895264, -0.01547494437545538, -0.04887561500072479, 0.0066654677502810955, -0.02191622368991375, 0.018206512555480003, -0.01069212332367897, 0.034565940499305725, 0.009378734044730663, -0.03253593295812607, 0.022740796208381653, -0.009458569809794426, -0.0017674140399321914, -0.02102900668978691, -0.018255021423101425, 0.006122589576989412, -0.010624842718243599, 0.03985099866986275, -0.005373913329094648, -0.016444548964500427, 0.001193528762087226, 1.4880493447450584e-34, 0.02947942353785038, -0.08276406675577164, -0.030582914128899574, 0.03151364624500275, -0.02054496668279171, -0.011866721324622631, 0.06457982212305069, 0.023742800578475, -0.006846690084785223, -0.019724447280168533, -0.012841646559536457]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "hf = sentence_transform_HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "text = \"This is a sample text.\"\n",
    "normalized_embedding = hf.embed_query(text)\n",
    "print(normalized_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sntnc_trnsfrmr_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Input data\n",
    "qa_list = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"Who wrote 'Harry Potter'?\", \"answer\": \"J.K. Rowling\"},\n",
    "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
    "    {\"question\": \"How many continents are there on Earth?\", \"answer\": \"7\"}\n",
    "]\n",
    "\n",
    "# Create fake_whole_documents and docs\n",
    "whole_docs = []\n",
    "vector_store_docs = []\n",
    "\n",
    "for qa in qa_list:\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    whole_docs.append((unique_id, Document(page_content=str(qa))))\n",
    "    vector_store_docs.append(Document(page_content=qa[\"question\"], metadata={\"doc_id\": unique_id}))\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.mset(whole_docs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b028de01-383f-4544-baa6-d647b872642e',\n",
       " '3c8a549a-f7b1-420d-9a40-007d4ceb5eaa',\n",
       " '88f3ca44-b53d-4476-a76e-66330d05ed58',\n",
       " '4165b175-ede7-45ff-94f7-1e313dcff1cc',\n",
       " 'c3009076-f67a-4d54-b9c8-fa5532cbb46c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline_retriver.add_documents(vector_store_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(vector_store_docs, hf)\n",
    "# print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CustomMultiVectorRetriever(vectorstore=db, docstore=docstore)\n",
    "# retriever.invoke(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'sub_docs': [Document(id='bd95c5d1-05f3-48c7-8ec4-ed203e402271', metadata={'doc_id': '53827114-9e99-4f8f-91c2-cc3850b921bf', 'score': 0.88379633}, page_content='What is the capital of France?')]}, page_content=\"{'question': 'What is the capital of France?', 'answer': 'Paris'}\"),\n",
       " Document(metadata={'sub_docs': [Document(id='6bc65c5f-9916-4059-b76b-be51f56cf8d5', metadata={'doc_id': '865bf635-dcb0-4715-9798-ebbbc8d00d62', 'score': 1.5906882}, page_content='How many continents are there on Earth?')]}, page_content=\"{'question': 'How many continents are there on Earth?', 'answer': '7'}\"),\n",
       " Document(metadata={'sub_docs': [Document(id='d7c56d25-e660-42b2-ad7a-eb69c2835d0f', metadata={'doc_id': '241cc491-9883-48c6-8ec1-64c8fc2b987f', 'score': 1.7001126}, page_content='What is the largest planet in our solar system?')]}, page_content=\"{'question': 'What is the largest planet in our solar system?', 'answer': 'Jupiter'}\"),\n",
       " Document(metadata={'sub_docs': [Document(id='a699aa20-796a-44df-95c7-ca3e767eff81', metadata={'doc_id': '1fbedf2f-a34f-472c-a315-ae8bc3a89de7', 'score': 1.9531672}, page_content='What is 2 + 2?')]}, page_content=\"{'question': 'What is 2 + 2?', 'answer': '4'}\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"capital of framce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[0.06765688210725784, 0.06349599361419678, 0.048713114112615585, 0.07930494844913483, 0.037448108196258545, 0.002652810188010335, 0.03937501087784767, -0.007098484318703413, 0.05936140939593315, 0.03153699263930321, 0.06009810045361519, -0.05290517583489418, 0.04060674458742142, -0.025930875912308693, 0.02984284609556198, 0.0011269178939983249, 0.07351484894752502, -0.05038188770413399, -0.1223866194486618, 0.023702865466475487, 0.029726574197411537, 0.04247688874602318, 0.025633757933974266, 0.0019951702561229467, -0.05691905692219734, -0.027159806340932846, -0.032903529703617096, 0.06602489203214645, 0.11900719255208969, -0.04587910696864128, -0.07262146472930908, -0.032584041357040405, 0.05234133452177048, 0.04505535587668419, 0.008253016509115696, 0.03670240566134453, -0.013941574841737747, 0.06539184600114822, -0.02642722986638546, 0.0002064133295789361, -0.01366433035582304, -0.0362810455262661, -0.019504407420754433, -0.028973795473575592, 0.039427030831575394, -0.08840904384851456, 0.0026242260355502367, 0.013671341352164745, 0.048306241631507874, -0.0311566311866045, -0.1173291727900505, -0.0511690117418766, -0.08852879703044891, -0.021896272897720337, 0.014298648573458195, 0.04441680759191513, -0.0134815638884902, 0.07433921098709106, 0.02663826197385788, -0.019876249134540558, 0.0179191492497921, -0.010605215094983578, -0.09042626619338989, 0.021326910704374313, 0.1412048488855362, -0.006471759174019098, -0.0014037804212421179, -0.015360948629677296, -0.08735720068216324, 0.07221739739179611, 0.020140305161476135, 0.04255874082446098, -0.034901391714811325, 0.0003194480377715081, -0.08029709756374359, -0.03274724632501602, 0.02852683886885643, -0.05136578157544136, 0.10938917845487595, 0.08193277567625046, -0.09840399771928787, -0.09340962767601013, -0.015129202976822853, 0.045124880969524384, 0.04941722750663757, -0.025186799466609955, 0.01570773869752884, -0.12929074466228485, 0.0053189038299024105, 0.004023429471999407, -0.02345716767013073, -0.0672982856631279, 0.029228007420897484, -0.02608451247215271, 0.013062501326203346, -0.031166262924671173, -0.04827136546373367, -0.05588594079017639, -0.03875049948692322, 0.1200108528137207, -0.010392437689006329, 0.04897051677107811, 0.055353663861751556, 0.044935811311006546, -0.0040097832679748535, -0.10295972973108292, -0.02929685451090336, -0.05834030359983444, 0.027047256007790565, -0.022016901522874832, -0.07222413271665573, -0.04138700291514397, -0.019329801201820374, 0.0027333248872309923, 0.0002770166320260614, -0.09675886482000351, -0.10057469457387924, -0.01419228408485651, -0.08078920841217041, 0.045392509549856186, 0.024504128843545914, 0.05976136401295662, -0.07381848990917206, 0.0119844451546669, -0.06634034961462021, -0.07690450549125671, 0.03851582109928131, -5.593623663850441e-33, 0.028001390397548676, -0.05607852712273598, -0.048660166561603546, 0.021556900814175606, 0.06019807234406471, -0.048140302300453186, -0.03502470999956131, 0.019331427291035652, -0.01751522906124592, -0.038921013474464417, -0.003810626221820712, -0.017028778791427612, 0.028209974989295006, 0.012829115614295006, 0.04716009646654129, 0.06210305541753769, -0.06435887515544891, 0.12928561866283417, -0.013123102486133575, 0.05230693891644478, -0.03736807033419609, 0.02890949696302414, -0.01689809188246727, -0.02373306266963482, -0.03334919363260269, -0.05167628452181816, 0.015535669401288033, 0.02088027074933052, -0.012537214905023575, 0.04595789685845375, 0.037271954119205475, 0.02805667743086815, -0.059000492095947266, -0.011698829010128975, 0.049218304455280304, 0.04703294113278389, 0.07354876399040222, -0.03705299645662308, 0.003984568640589714, 0.010641198605298996, -0.00016147120913956314, -0.05271657928824425, 0.02759280800819397, -0.0392921082675457, 0.08447176963090897, 0.04868607968091965, -0.004858717788010836, 0.017994895577430725, -0.04285696893930435, 0.01233754027634859, 0.006399533245712519, 0.040482331067323685, 0.014888713136315346, -0.015394152142107487, 0.07629487663507462, 0.023704426363110542, 0.044523708522319794, 0.050819627940654755, -0.002312537282705307, -0.0188736729323864, -0.012333560734987259, 0.04660015553236008, -0.05634377524256706, 0.06299278885126114, -0.03155350685119629, 0.03249122202396393, 0.023467333987355232, -0.06554382294416428, 0.02017093077301979, 0.025708258152008057, -0.012386927381157875, -0.008364882320165634, -0.06643777340650558, 0.09430735558271408, -0.03570927307009697, -0.034248337149620056, -0.006663557142019272, -0.008015280589461327, -0.03097112663090229, 0.04330125451087952, -0.00821398850530386, -0.15079504251480103, 0.03076920285820961, 0.04007190093398094, -0.037929337471723557, 0.0019321560394018888, 0.04005309194326401, -0.08770751953125, -0.036849040538072586, 0.008579595945775509, -0.03192517161369324, -0.012525752186775208, 0.07355399429798126, 0.001347402692772448, 0.02059186063706875, 2.710982928228939e-33, -0.05185771733522415, 0.05783611536026001, -0.09189854562282562, 0.03944212943315506, 0.1055765226483345, -0.01969117857515812, 0.0618402473628521, -0.0763465091586113, 0.024088043719530106, 0.09400489181280136, -0.11653542518615723, 0.03711983188986778, 0.05224251002073288, -0.003958548419177532, 0.057221438735723495, 0.00532852578908205, 0.12401685118675232, 0.013902222737669945, -0.01102503016591072, 0.035605330020189285, -0.033075470477342606, 0.08165743201971054, -0.01520040538161993, 0.060558516532182693, -0.060139741748571396, 0.032610245048999786, -0.03482966497540474, -0.01698814332485199, -0.09749073535203934, -0.02714846469461918, 0.0017471375176683068, -0.07689822465181351, -0.04318580403923988, -0.01899845339357853, -0.029166122898459435, 0.057748790830373764, 0.024182165041565895, -0.011690289713442326, -0.06214352697134018, 0.02843518555164337, -0.00023750573745928705, -0.025178339332342148, 0.004396399948745966, 0.08128400146961212, 0.036418430507183075, -0.06040063500404358, -0.036551717668771744, -0.07937485724687576, -0.005085248965770006, 0.0669698715209961, -0.11778434365987778, 0.03237432613968849, -0.04712529480457306, -0.013445930555462837, -0.09484448283910751, 0.008249538950622082, -0.010674898512661457, -0.06818816810846329, 0.0011181208537891507, 0.024802053347229958, -0.0635889321565628, 0.02844933792948723, -0.026130391284823418, 0.0858110636472702, 0.11468227207660675, -0.05353453382849693, -0.05635887756943703, 0.042600926011800766, 0.010945456102490425, 0.020957937464118004, 0.10013113915920258, 0.03260508552193642, -0.18420878052711487, -0.03932088613510132, -0.06914550811052322, -0.06381049752235413, -0.06563860923051834, -0.0064125363714993, -0.04796125739812851, -0.07681333273649216, 0.029538433998823166, -0.022994806990027428, 0.0417037308216095, -0.025004755705595016, -0.004545089788734913, -0.04171368479728699, -0.013228987343609333, -0.06383569538593292, -0.0024647247046232224, -0.013733768835663795, 0.016897669062018394, -0.06303980946540833, 0.08988809585571289, 0.04181710258126259, -0.0185687355697155, -1.804421678741619e-08, -0.016799796372652054, -0.03215779736638069, 0.06303838640451431, -0.04130921512842178, 0.04448186233639717, 0.0020246016792953014, 0.06295926123857498, -0.005173712968826294, -0.010044433176517487, -0.030563971027731895, 0.03526727110147476, 0.05585819110274315, -0.046712473034858704, 0.034510280936956406, 0.0329577773809433, 0.04301145300269127, 0.029436076059937477, -0.030316544696688652, -0.017110802233219147, 0.07374847680330276, -0.05479102581739426, 0.027751507237553596, 0.00620166165754199, 0.0158800408244133, 0.0342978835105896, -0.005157511215656996, 0.023507975041866302, 0.07531353831291199, 0.019284335896372795, 0.03361964970827103, 0.050910383462905884, 0.1524970531463623, 0.016420789062976837, 0.02705284394323826, 0.037516213953495026, 0.021855320781469345, 0.05663339048624039, -0.039574623107910156, 0.07123130559921265, -0.0541376955807209, 0.0010376281570643187, 0.02118527889251709, -0.0356309600174427, 0.10901692509651184, 0.0027653402648866177, 0.03139973059296608, 0.0013841877225786448, -0.034573912620544434, -0.04592779651284218, 0.028808271512389183, 0.007169041782617569, 0.0484684519469738, 0.026101818308234215, -0.009440738707780838, 0.028216945007443428, 0.034872401505708694, 0.036909881979227066, -0.008589518256485462, -0.03532052040100098, -0.0247857216745615, -0.01919209584593773, 0.03807077184319496, 0.05996536463499069, -0.04222874715924263], [0.08643858879804611, 0.10276263952255249, 0.005394589155912399, 0.002044407417997718, -0.009963334538042545, 0.0253855399787426, 0.049287546426057816, -0.030626554042100906, 0.06872548162937164, 0.01013659592717886, 0.07753980904817581, -0.09008073806762695, 0.0061062052845954895, -0.0569898821413517, 0.014171456918120384, 0.028049170970916748, -0.0868464782834053, 0.07643991708755493, -0.10349133610725403, -0.06774378567934036, 0.06999467313289642, 0.08442512899637222, -0.007249127142131329, 0.010477058589458466, 0.013402045704424381, 0.06775771081447601, -0.09420862793922424, -0.037169016897678375, 0.05226175859570503, -0.031085336580872536, -0.09634070843458176, 0.015771690756082535, 0.02578669786453247, 0.07852450013160706, 0.07899492979049683, 0.01915162429213524, 0.016435639932751656, 0.0031008601654320955, 0.03813116252422333, 0.02370912954211235, 0.010538970120251179, -0.04406449943780899, 0.044173870235681534, -0.02587283030152321, 0.06153789162635803, -0.04054277017712593, -0.08641398698091507, 0.0319722481071949, -0.0008906704606488347, -0.024443743750452995, -0.09197209775447845, 0.02339397743344307, -0.08302931487560272, 0.04415106028318405, -0.024969283491373062, 0.06230199709534645, -0.0013035606825724244, 0.07513952255249023, 0.024638494476675987, -0.06472446024417877, -0.11772772669792175, 0.03833920881152153, -0.09117674827575684, 0.06354460120201111, 0.07627400010824203, -0.08802413195371628, 0.00954559538513422, -0.046971794217824936, -0.08417405188083649, 0.03888238966464996, -0.11439356952905655, 0.0062885889783501625, -0.03493615612387657, 0.023975033313035965, -0.033131685107946396, -0.015724359080195427, -0.037895567715168, -0.008812500163912773, 0.07061192393302917, 0.0328066423535347, 0.002036717254668474, -0.11227897554636002, 0.006797217298299074, 0.012276574969291687, 0.033530350774526596, -0.01362010557204485, -0.02254902571439743, -0.02252287231385708, -0.02031947672367096, 0.05042976886034012, -0.07486531883478165, -0.08228221535682678, 0.07659624516963959, 0.049339208751916885, -0.03755536302924156, 0.014463438652455807, -0.05724579840898514, -0.017995445057749748, 0.10969793796539307, 0.11946279555559158, 0.0008092325879260898, 0.06170574203133583, 0.03263222426176071, -0.13078011572360992, -0.14863663911819458, -0.06162325665354729, 0.04338861256837845, 0.026712898164987564, 0.013978637754917145, -0.03940022364258766, -0.02527119591832161, 0.0038773997221142054, 0.03586646914482117, -0.06154203042387962, 0.03766606003046036, 0.02675652876496315, -0.038265932351350784, -0.035479314625263214, -0.02392273023724556, 0.08679773658514023, -0.01840629056096077, 0.07710393518209457, 0.0013986396370455623, 0.0700383186340332, -0.04778776690363884, -0.07898197323083878, 0.051081474870443344, -2.9986833338827707e-33, -0.039164647459983826, -0.0025621242821216583, 0.016521085053682327, 0.009489381685853004, -0.056621916592121124, 0.06577832251787186, -0.04770028218626976, 0.011166181415319443, -0.057355839759111404, -0.009162593632936478, -0.021752120926976204, -0.05595313757658005, -0.011142294853925705, 0.09327931702136993, 0.016676515340805054, -0.01367234718054533, 0.0434388592839241, 0.0018724261317402124, 0.007299462333321571, 0.05163317918777466, 0.04806084185838699, 0.13534143567085266, -0.01717388816177845, -0.012969853356480598, -0.0750110000371933, 0.026110751554369926, 0.02698022499680519, 0.0007830616668798029, -0.04872703552246094, 0.011784270405769348, -0.04595807194709778, -0.048321355134248734, -0.019567087292671204, 0.019388949498534203, 0.019880736246705055, 0.01674322597682476, 0.09878010302782059, -0.027408774942159653, 0.023480897769331932, 0.0037022733595222235, -0.06145147979259491, -0.001212272560223937, -0.009504728950560093, 0.009251547046005726, 0.023844389244914055, 0.08612319827079773, 0.022678976878523827, 0.0005451350589282811, 0.034712936729192734, 0.006254659499973059, -0.00692776869982481, 0.0392400324344635, 0.011567501351237297, 0.03262801468372345, 0.06221552938222885, 0.027611423283815384, 0.018688349053263664, 0.03558061271905899, 0.0411796048283577, 0.015478230081498623, 0.042269136756658554, 0.038224831223487854, 0.010031343437731266, -0.028324592858552933, 0.044705234467983246, -0.04104587063193321, -0.004505471792072058, -0.054473403841257095, 0.026232073083519936, 0.01798625849187374, -0.12311874330043793, -0.04669518396258354, -0.013591345399618149, 0.06467105448246002, 0.003573488909751177, -0.012223378755152225, -0.017938219010829926, -0.025550220161676407, 0.023722419515252113, 0.004086659289896488, -0.06514763087034225, 0.044365156441926956, 0.046859610825777054, -0.03251749649643898, 0.004022690001875162, -0.003976042848080397, 0.011193969286978245, -0.09955978393554688, 0.03331688046455383, 0.08010607957839966, 0.09426922351121902, -0.06382941454648972, 0.03231515735387802, -0.05135535076260567, -0.007498799357563257, 5.30048769885001e-34, -0.04131951183080673, 0.09496468305587769, -0.10640142112970352, 0.049659062176942825, -0.0341913141310215, -0.0316745899617672, -0.017155617475509644, 0.0017010465962812304, 0.05797579884529114, -0.001217756769619882, -0.016853632405400276, -0.05169127136468887, 0.05529988557100296, -0.034264788031578064, 0.030817953869700432, -0.031048079952597618, 0.09275326877832413, 0.03726636990904808, -0.023739822208881378, 0.04458937793970108, 0.014615342020988464, 0.11623936146497726, -0.050011277198791504, 0.038871657103300095, 0.0042474293150007725, 0.02569766156375408, 0.03272440657019615, 0.04299076274037361, -0.013614445924758911, 0.025612205266952515, 0.010626222006976604, -0.08468639850616455, -0.09529823064804077, 0.1083998903632164, -0.07516001164913177, -0.013777351938188076, 0.06373383849859238, -0.004496716894209385, -0.032532162964344025, 0.06236138194799423, 0.03480532765388489, -0.03549225255846977, -0.020022273063659668, 0.03666084632277489, -0.024883713573217392, 0.010181860066950321, -0.07012332230806351, -0.04319510981440544, 0.02953328564763069, -0.00029488845029845834, -0.034538693726062775, 0.014667601324617863, -0.09839700907468796, -0.04704879969358444, -0.008854977786540985, -0.08899140357971191, 0.03509959951043129, -0.12960198521614075, -0.04988664761185646, -0.06120474636554718, -0.05977979302406311, 0.009463176131248474, 0.04912179335951805, -0.07750265300273895, 0.08097270876169205, -0.04792572930455208, 0.002343789441511035, 0.07570314407348633, -0.024017570540308952, -0.015254605561494827, 0.04867386072874069, -0.038596827536821365, -0.07048317790031433, -0.012034826911985874, -0.03887907788157463, -0.07760170847177505, -0.01072437409311533, 0.01041884534060955, -0.021375400945544243, -0.09173864126205444, -0.011134488508105278, -0.029606610536575317, 0.02464584819972515, 0.004657107405364513, -0.016344984993338585, -0.03952197730541229, 0.07733739167451859, -0.028473295271396637, -0.0036993911489844322, 0.08276653289794922, -0.011040918529033661, 0.031398314982652664, 0.05350945144891739, 0.057514581829309464, -0.031762219965457916, -1.5291126587158033e-08, -0.07996615022420883, -0.04767969995737076, -0.0859789103269577, 0.05696168169379234, -0.04088665917515755, 0.02238324284553528, -0.004644492641091347, -0.0380130298435688, -0.031067106872797012, -0.010727766901254654, 0.01976986788213253, 0.007770010735839605, -0.0060947248712182045, -0.03863762691617012, 0.028027193620800972, 0.06781377643346786, -0.023535167798399925, 0.03217474743723869, 0.008025371469557285, -0.023910721763968468, -0.001219946308992803, 0.03145990148186684, -0.0524924136698246, -0.008068159222602844, 0.0031477513257414103, 0.051149632781744, -0.04441043734550476, 0.06360135227441788, 0.03850838914513588, 0.033043280243873596, -0.004187280312180519, 0.049559250473976135, -0.05696052312850952, -0.00649713771417737, -0.02497929334640503, -0.016086719930171967, 0.06622893363237381, -0.020631078630685806, 0.10804574191570282, 0.016854727640748024, 0.014381224289536476, -0.01321274135261774, -0.12938737869262695, 0.06952163577079773, -0.05557730048894882, -0.06754133105278015, -0.005458198022097349, -0.006135937292128801, 0.03908409923315048, -0.06287796795368195, 0.03740633651614189, -0.011657067574560642, 0.012915012426674366, -0.05524953082203865, 0.05160759761929512, -0.004308419767767191, 0.05802478641271591, 0.018694501370191574, 0.022781040519475937, 0.032166559249162674, 0.0537978857755661, 0.07028491050004959, 0.07493121176958084, -0.08417750149965286]]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "class ParrotLinkEmbeddings(Embeddings):\n",
    "    \"\"\"ParrotLink embedding model integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        normalized_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return normalized_embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "    # Optional: add custom async implementations here\n",
    "    # you can also delete these, and the base class will\n",
    "    # use the default implementation, which calls the sync\n",
    "    # version in an async executor:\n",
    "\n",
    "    # async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "    #     \"\"\"Asynchronous Embed search docs.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "    # async def aembed_query(self, text: str) -> List[float]:\n",
    "    #     \"\"\"Asynchronous Embed query text.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "# Example usage\n",
    "embed = ParrotLinkEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "sentence_embeddings = embed.embed_documents(sentences)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2:1b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Input data\n",
    "qa_list = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"Who wrote 'Harry Potter'?\", \"answer\": \"J.K. Rowling\"},\n",
    "    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n",
    "    {\"question\": \"How many continents are there on Earth?\", \"answer\": \"7\"}\n",
    "]\n",
    "\n",
    "# Create fake_whole_documents and docs\n",
    "whole_docs = []\n",
    "vector_store_docs = []\n",
    "\n",
    "for qa in qa_list:\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    whole_docs.append((unique_id, Document(page_content=str(qa))))\n",
    "    vector_store_docs.append(Document(page_content=qa[\"question\"], metadata={\"doc_id\": unique_id}))\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.mset(whole_docs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(vector_store_docs, embed)\n",
    "# print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='52baf48e-9e85-427a-a919-5a4ac5d11a5c', metadata={'doc_id': 'b3841765-ce7c-4681-a1d0-be9b5e57f53a'}, page_content='What is the capital of France?'),\n",
       "  2.3202433e-13),\n",
       " (Document(id='09250829-6932-4c2f-8455-4e051917e529', metadata={'doc_id': '5a4b82ad-e27a-4c11-82fa-8125d9dc5ac8'}, page_content='How many continents are there on Earth?'),\n",
       "  1.4522676),\n",
       " (Document(id='68b79531-61e9-466e-a537-2f61016b7cbe', metadata={'doc_id': 'edf589d4-cc09-4404-8c6a-4404d87c14db'}, page_content='What is the largest planet in our solar system?'),\n",
       "  1.6004306)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(\"What is the capital of France?\", search_type=\"mmr\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrm_b = Chroma.from_documents(documents=vector_store_docs, embedding=embed, collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sntnc_trnsfrmr_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrm_b_senc = Chroma.from_documents(documents=vector_store_docs, embedding=sntnc_trnsfrmr_embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='05701885-a5c2-44e6-9330-8794fa303ba7', metadata={'doc_id': 'b3841765-ce7c-4681-a1d0-be9b5e57f53a'}, page_content='What is the capital of France?'),\n",
       "  0.09321856498718262),\n",
       " (Document(id='c6f3e2e9-c50a-4596-8589-f0a54eee3919', metadata={'doc_id': 'b3841765-ce7c-4681-a1d0-be9b5e57f53a'}, page_content='What is the capital of France?'),\n",
       "  0.09321862459182739),\n",
       " (Document(id='b4576440-f3fb-47dc-ae81-c9b16e46c676', metadata={'doc_id': '5a4b82ad-e27a-4c11-82fa-8125d9dc5ac8'}, page_content='How many continents are there on Earth?'),\n",
       "  0.881251871585846),\n",
       " (Document(id='c9c9244b-20b0-4c9f-94c4-d7f7f2f00e10', metadata={'doc_id': '5a4b82ad-e27a-4c11-82fa-8125d9dc5ac8'}, page_content='How many continents are there on Earth?'),\n",
       "  0.8812519311904907)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrm_b_senc.similarity_search_with_score(\"france capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "retriever = CustomMultiVectorRetriever(vectorstore=db, docstore=docstore)\n",
    "retriever.invoke(\"capital of france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Ed\\RAGs\\fortive_rag_v1\\Multimodal_RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Ed\\HF_Models\\hub\\models--mixedbread-ai--mxbai-embed-xsmall-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "sntnc_trnsfrmr_embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(vector_store_docs, sntnc_trnsfrmr_embeddings)\n",
    "# print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='ea108920-fd9a-4d38-9543-8f311b3d8a90', metadata={'doc_id': 'b3841765-ce7c-4681-a1d0-be9b5e57f53a'}, page_content='What is the capital of France?'),\n",
       "  9.476099),\n",
       " (Document(id='0164f811-a39f-4004-ba62-bd1069e0cf72', metadata={'doc_id': '5a4b82ad-e27a-4c11-82fa-8125d9dc5ac8'}, page_content='How many continents are there on Earth?'),\n",
       "  70.07296),\n",
       " (Document(id='089e32db-af29-4133-8247-25d5f747be79', metadata={'doc_id': 'edf589d4-cc09-4404-8c6a-4404d87c14db'}, page_content='What is the largest planet in our solar system?'),\n",
       "  76.16902),\n",
       " (Document(id='61b036d1-5bfc-4bdd-8895-bca7545133d3', metadata={'doc_id': '948677c9-39af-4c37-b8e2-673631957a4a'}, page_content=\"Who wrote 'Harry Potter'?\"),\n",
       "  80.85421)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score(\"france capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarities: tensor([[ 0.4704, -0.0075,  0.0422,  0.0438]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# 1. Specify preferred dimensions\n",
    "dimensions = 384\n",
    "\n",
    "# 2. Load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-xsmall-v1\", truncate_dim=dimensions)\n",
    "\n",
    "query = 'fance capital'\n",
    "\n",
    "docs = [\n",
    "    query,\n",
    "    \"what is the capital of france?\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "\n",
    "# 3. Encode\n",
    "embeddings = model.encode(docs)\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n",
      "          2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n",
      "          6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n",
      "          1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n",
      "          2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n",
      "         -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n",
      "         -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2530e-03,\n",
      "          3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0641e-04,\n",
      "         -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n",
      "         -8.8409e-02,  2.6242e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n",
      "         -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n",
      "          4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n",
      "          1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n",
      "         -6.4718e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n",
      "          2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1945e-04, -8.0297e-02,\n",
      "         -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n",
      "         -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n",
      "         -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n",
      "         -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3063e-02,\n",
      "         -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8750e-02,  1.2001e-01,\n",
      "         -1.0392e-02,  4.8971e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n",
      "         -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n",
      "         -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7702e-04,\n",
      "         -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5393e-02,\n",
      "          2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n",
      "         -7.6905e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6079e-02,\n",
      "         -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n",
      "          1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8106e-03, -1.7029e-02,\n",
      "          2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n",
      "          1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n",
      "         -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n",
      "          2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n",
      "         -5.9000e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n",
      "         -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6147e-04, -5.2717e-02,\n",
      "          2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n",
      "          1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n",
      "          1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n",
      "          5.0820e-02, -2.3125e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n",
      "         -5.6344e-02,  6.2993e-02, -3.1554e-02,  3.2491e-02,  2.3467e-02,\n",
      "         -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n",
      "         -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n",
      "         -8.0153e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5080e-01,\n",
      "          3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n",
      "         -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n",
      "          7.3554e-02,  1.3474e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n",
      "          5.7836e-02, -9.1899e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n",
      "          6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n",
      "          3.7120e-02,  5.2243e-02, -3.9585e-03,  5.7221e-02,  5.3285e-03,\n",
      "          1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n",
      "          8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n",
      "         -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n",
      "         -7.6898e-02, -4.3186e-02, -1.8998e-02, -2.9166e-02,  5.7749e-02,\n",
      "          2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3751e-04,\n",
      "         -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n",
      "         -3.6552e-02, -7.9375e-02, -5.0852e-03,  6.6970e-02, -1.1778e-01,\n",
      "          3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2495e-03,\n",
      "         -1.0675e-02, -6.8188e-02,  1.1181e-03,  2.4802e-02, -6.3589e-02,\n",
      "          2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n",
      "         -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n",
      "          3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9146e-02, -6.3810e-02,\n",
      "         -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n",
      "         -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5451e-03, -4.1714e-02,\n",
      "         -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n",
      "         -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n",
      "         -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n",
      "          2.0246e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n",
      "          3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n",
      "          4.3011e-02,  2.9436e-02, -3.0317e-02, -1.7111e-02,  7.3748e-02,\n",
      "         -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n",
      "         -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n",
      "          5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n",
      "          2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n",
      "          1.0376e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n",
      "          3.1400e-02,  1.3842e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n",
      "          7.1690e-03,  4.8468e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n",
      "          3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n",
      "         -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02],\n",
      "        [ 8.6439e-02,  1.0276e-01,  5.3946e-03,  2.0444e-03, -9.9633e-03,\n",
      "          2.5386e-02,  4.9288e-02, -3.0627e-02,  6.8725e-02,  1.0137e-02,\n",
      "          7.7540e-02, -9.0081e-02,  6.1062e-03, -5.6990e-02,  1.4171e-02,\n",
      "          2.8049e-02, -8.6846e-02,  7.6440e-02, -1.0349e-01, -6.7744e-02,\n",
      "          6.9995e-02,  8.4425e-02, -7.2491e-03,  1.0477e-02,  1.3402e-02,\n",
      "          6.7758e-02, -9.4209e-02, -3.7169e-02,  5.2262e-02, -3.1085e-02,\n",
      "         -9.6341e-02,  1.5772e-02,  2.5787e-02,  7.8525e-02,  7.8995e-02,\n",
      "          1.9152e-02,  1.6436e-02,  3.1009e-03,  3.8131e-02,  2.3709e-02,\n",
      "          1.0539e-02, -4.4064e-02,  4.4174e-02, -2.5873e-02,  6.1538e-02,\n",
      "         -4.0543e-02, -8.6414e-02,  3.1972e-02, -8.9067e-04, -2.4444e-02,\n",
      "         -9.1972e-02,  2.3394e-02, -8.3029e-02,  4.4151e-02, -2.4969e-02,\n",
      "          6.2302e-02, -1.3036e-03,  7.5140e-02,  2.4638e-02, -6.4724e-02,\n",
      "         -1.1773e-01,  3.8339e-02, -9.1177e-02,  6.3545e-02,  7.6274e-02,\n",
      "         -8.8024e-02,  9.5456e-03, -4.6972e-02, -8.4174e-02,  3.8882e-02,\n",
      "         -1.1439e-01,  6.2886e-03, -3.4936e-02,  2.3975e-02, -3.3132e-02,\n",
      "         -1.5724e-02, -3.7896e-02, -8.8125e-03,  7.0612e-02,  3.2807e-02,\n",
      "          2.0367e-03, -1.1228e-01,  6.7972e-03,  1.2277e-02,  3.3530e-02,\n",
      "         -1.3620e-02, -2.2549e-02, -2.2523e-02, -2.0319e-02,  5.0430e-02,\n",
      "         -7.4865e-02, -8.2282e-02,  7.6596e-02,  4.9339e-02, -3.7555e-02,\n",
      "          1.4463e-02, -5.7246e-02, -1.7995e-02,  1.0970e-01,  1.1946e-01,\n",
      "          8.0923e-04,  6.1706e-02,  3.2632e-02, -1.3078e-01, -1.4864e-01,\n",
      "         -6.1623e-02,  4.3389e-02,  2.6713e-02,  1.3979e-02, -3.9400e-02,\n",
      "         -2.5271e-02,  3.8774e-03,  3.5866e-02, -6.1542e-02,  3.7666e-02,\n",
      "          2.6757e-02, -3.8266e-02, -3.5479e-02, -2.3923e-02,  8.6798e-02,\n",
      "         -1.8406e-02,  7.7104e-02,  1.3986e-03,  7.0038e-02, -4.7788e-02,\n",
      "         -7.8982e-02,  5.1081e-02, -2.9987e-33, -3.9165e-02, -2.5621e-03,\n",
      "          1.6521e-02,  9.4894e-03, -5.6622e-02,  6.5778e-02, -4.7700e-02,\n",
      "          1.1166e-02, -5.7356e-02, -9.1626e-03, -2.1752e-02, -5.5953e-02,\n",
      "         -1.1142e-02,  9.3279e-02,  1.6677e-02, -1.3672e-02,  4.3439e-02,\n",
      "          1.8724e-03,  7.2995e-03,  5.1633e-02,  4.8061e-02,  1.3534e-01,\n",
      "         -1.7174e-02, -1.2970e-02, -7.5011e-02,  2.6111e-02,  2.6980e-02,\n",
      "          7.8306e-04, -4.8727e-02,  1.1784e-02, -4.5958e-02, -4.8321e-02,\n",
      "         -1.9567e-02,  1.9389e-02,  1.9881e-02,  1.6743e-02,  9.8780e-02,\n",
      "         -2.7409e-02,  2.3481e-02,  3.7023e-03, -6.1451e-02, -1.2123e-03,\n",
      "         -9.5047e-03,  9.2515e-03,  2.3844e-02,  8.6123e-02,  2.2679e-02,\n",
      "          5.4514e-04,  3.4713e-02,  6.2547e-03, -6.9278e-03,  3.9240e-02,\n",
      "          1.1568e-02,  3.2628e-02,  6.2216e-02,  2.7611e-02,  1.8688e-02,\n",
      "          3.5581e-02,  4.1180e-02,  1.5478e-02,  4.2269e-02,  3.8225e-02,\n",
      "          1.0031e-02, -2.8325e-02,  4.4705e-02, -4.1046e-02, -4.5055e-03,\n",
      "         -5.4473e-02,  2.6232e-02,  1.7986e-02, -1.2312e-01, -4.6695e-02,\n",
      "         -1.3591e-02,  6.4671e-02,  3.5735e-03, -1.2223e-02, -1.7938e-02,\n",
      "         -2.5550e-02,  2.3722e-02,  4.0867e-03, -6.5148e-02,  4.4365e-02,\n",
      "          4.6860e-02, -3.2517e-02,  4.0227e-03, -3.9760e-03,  1.1194e-02,\n",
      "         -9.9560e-02,  3.3317e-02,  8.0106e-02,  9.4269e-02, -6.3829e-02,\n",
      "          3.2315e-02, -5.1355e-02, -7.4988e-03,  5.3005e-34, -4.1320e-02,\n",
      "          9.4965e-02, -1.0640e-01,  4.9659e-02, -3.4191e-02, -3.1675e-02,\n",
      "         -1.7156e-02,  1.7010e-03,  5.7976e-02, -1.2178e-03, -1.6854e-02,\n",
      "         -5.1691e-02,  5.5300e-02, -3.4265e-02,  3.0818e-02, -3.1048e-02,\n",
      "          9.2753e-02,  3.7266e-02, -2.3740e-02,  4.4589e-02,  1.4615e-02,\n",
      "          1.1624e-01, -5.0011e-02,  3.8872e-02,  4.2474e-03,  2.5698e-02,\n",
      "          3.2724e-02,  4.2991e-02, -1.3614e-02,  2.5612e-02,  1.0626e-02,\n",
      "         -8.4686e-02, -9.5298e-02,  1.0840e-01, -7.5160e-02, -1.3777e-02,\n",
      "          6.3734e-02, -4.4967e-03, -3.2532e-02,  6.2361e-02,  3.4805e-02,\n",
      "         -3.5492e-02, -2.0022e-02,  3.6661e-02, -2.4884e-02,  1.0182e-02,\n",
      "         -7.0123e-02, -4.3195e-02,  2.9533e-02, -2.9489e-04, -3.4539e-02,\n",
      "          1.4668e-02, -9.8397e-02, -4.7049e-02, -8.8550e-03, -8.8991e-02,\n",
      "          3.5100e-02, -1.2960e-01, -4.9887e-02, -6.1205e-02, -5.9780e-02,\n",
      "          9.4632e-03,  4.9122e-02, -7.7503e-02,  8.0973e-02, -4.7926e-02,\n",
      "          2.3438e-03,  7.5703e-02, -2.4018e-02, -1.5255e-02,  4.8674e-02,\n",
      "         -3.8597e-02, -7.0483e-02, -1.2035e-02, -3.8879e-02, -7.7602e-02,\n",
      "         -1.0724e-02,  1.0419e-02, -2.1375e-02, -9.1739e-02, -1.1134e-02,\n",
      "         -2.9607e-02,  2.4646e-02,  4.6571e-03, -1.6345e-02, -3.9522e-02,\n",
      "          7.7337e-02, -2.8473e-02, -3.6994e-03,  8.2767e-02, -1.1041e-02,\n",
      "          3.1398e-02,  5.3509e-02,  5.7515e-02, -3.1762e-02, -1.5291e-08,\n",
      "         -7.9966e-02, -4.7680e-02, -8.5979e-02,  5.6962e-02, -4.0887e-02,\n",
      "          2.2383e-02, -4.6445e-03, -3.8013e-02, -3.1067e-02, -1.0728e-02,\n",
      "          1.9770e-02,  7.7700e-03, -6.0947e-03, -3.8638e-02,  2.8027e-02,\n",
      "          6.7814e-02, -2.3535e-02,  3.2175e-02,  8.0254e-03, -2.3911e-02,\n",
      "         -1.2199e-03,  3.1460e-02, -5.2492e-02, -8.0682e-03,  3.1478e-03,\n",
      "          5.1150e-02, -4.4410e-02,  6.3601e-02,  3.8508e-02,  3.3043e-02,\n",
      "         -4.1873e-03,  4.9559e-02, -5.6961e-02, -6.4971e-03, -2.4979e-02,\n",
      "         -1.6087e-02,  6.6229e-02, -2.0631e-02,  1.0805e-01,  1.6855e-02,\n",
      "          1.4381e-02, -1.3213e-02, -1.2939e-01,  6.9522e-02, -5.5577e-02,\n",
      "         -6.7541e-02, -5.4582e-03, -6.1359e-03,  3.9084e-02, -6.2878e-02,\n",
      "          3.7406e-02, -1.1657e-02,  1.2915e-02, -5.5250e-02,  5.1608e-02,\n",
      "         -4.3084e-03,  5.8025e-02,  1.8695e-02,  2.2781e-02,  3.2167e-02,\n",
      "          5.3798e-02,  7.0285e-02,  7.4931e-02, -8.4178e-02]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # Get token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences for embedding\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Get token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Apply mean pooling to get sentence embeddings\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize the embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# Output the embeddings\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(sentence_embeddings**2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = torch.tensor(sentence_embeddings)\n",
    "torch.sum(se**2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "from pathlib import Path\n",
    "import time \n",
    "\n",
    "llm_llama_gaurd3 = ChatOllama(\n",
    "    model=\"llama-guard3:1b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "def responsible_ai_validation(question):\n",
    "    llama_gaurd_labels = '''S1: Violent Crimes\n",
    "    S3: Sex-Related Crimes\n",
    "    S5: Defamation\n",
    "    S7: Privacy\n",
    "    S9: Indiscriminate Weapons\n",
    "    S11: Suicide & Self-Harm\n",
    "    S13: Elections\n",
    "    S2: Non-Violent Crimes\n",
    "    S4: Child Sexual Exploitation\n",
    "    S6: Specialized Advice\n",
    "    S8: Intellectual Property\n",
    "    S10: Hate\n",
    "    S12: Sexual Content\n",
    "    S14: Code Interpreter Abuse'''.splitlines()\n",
    "    llama_gaurd_label_dict = dict([vl.strip().split(\": \") for vl in llama_gaurd_labels])\n",
    "    grd_rslt = llm_llama_gaurd3.invoke(question).content.splitlines()\n",
    "    if len(grd_rslt) > 1:\n",
    "        llama_gaurd_rslt = llama_gaurd_label_dict[grd_rslt.content.splitlines()[-1]]\n",
    "        return f'Your prompt includes {grd_rslt[-1]} request. Please rephrase your query'\n",
    "    else:\n",
    "        return \"safe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='unsafe\\nS11', additional_kwargs={}, response_metadata={'model': 'llama-guard3:1b', 'created_at': '2025-01-15T14:11:19.2400129Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2376143700, 'load_duration': 28341900, 'prompt_eval_count': 198, 'prompt_eval_duration': 783000000, 'eval_count': 5, 'eval_duration': 1562000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ca9f900a-704c-40dc-b4f3-5df2411b560f-0', usage_metadata={'input_tokens': 198, 'output_tokens': 5, 'total_tokens': 203})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grd_rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['safe']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_llama_gaurd3.invoke(\"how are you?\").content.splitlines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
